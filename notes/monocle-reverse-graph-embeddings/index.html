<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Monocle - Reverse Graph Embeddings | </title>
<meta name="keywords" content="monocle, latent-model, dimensionality-reduction, reverse-graph-embeddings">
<meta name="description" content="Everything is Not What They Seem More than just a throwback to Wizards of Waverly Place, this statement is fundamental to the way our world operates. Specifically, it hints at the existance of a hidden dimension, a latent space that exists beneath the surface of our reality. And in my opinion, I think that the models surrounding these latent spaces are quite neat: they hint at the idea that there exists a fundamental discrepancy between what is overtly shown and how these patterns come to be.">
<meta name="author" content="">
<link rel="canonical" href="https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.08f38febdde379994981340a0d46cdb7267a245120dc57269145d8524031b393.css" integrity="sha256-CPOP693jeZlJgTQKDUbNtyZ6JFEg3FcmkUXYUkAxs5M=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://sunnyson.dev/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://sunnyson.dev/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://sunnyson.dev/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://sunnyson.dev/apple-touch-icon.png">
<link rel="mask-icon" href="https://sunnyson.dev/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="apple-touch-icon" sizes="152x152" href="/path-to-your-icon/apple-touch-icon-152x152.png">
<link rel="apple-touch-icon" sizes="120x120" href="/path-to-your-icon/apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" sizes="76x76" href="/path-to-your-icon/apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" sizes="60x60" href="/path-to-your-icon/apple-touch-icon-60x60.png">













<style>
    @media screen and (min-width: 769px) {
         
        .post-content input[type="checkbox"]:checked ~ label > img {
            transform: scale(1.6);
            cursor: zoom-out;
            position: relative;
            z-index: 999;
        }

        .post-content img.zoomCheck {
            transition: transform 0.15s ease;
            z-index: 999;
            cursor: zoom-in;
        }
    }
</style>
<meta property="og:title" content="Monocle - Reverse Graph Embeddings" />
<meta property="og:description" content="Everything is Not What They Seem More than just a throwback to Wizards of Waverly Place, this statement is fundamental to the way our world operates. Specifically, it hints at the existance of a hidden dimension, a latent space that exists beneath the surface of our reality. And in my opinion, I think that the models surrounding these latent spaces are quite neat: they hint at the idea that there exists a fundamental discrepancy between what is overtly shown and how these patterns come to be." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/" />
<meta property="og:image" content="https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/images/dalle-white-ball.jpg" /><meta property="article:section" content="notes" />
<meta property="article:published_time" content="2023-07-23T07:00:00-04:00" />
<meta property="article:modified_time" content="2023-07-23T07:00:00-04:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/images/dalle-white-ball.jpg" />
<meta name="twitter:title" content="Monocle - Reverse Graph Embeddings"/>
<meta name="twitter:description" content="Everything is Not What They Seem More than just a throwback to Wizards of Waverly Place, this statement is fundamental to the way our world operates. Specifically, it hints at the existance of a hidden dimension, a latent space that exists beneath the surface of our reality. And in my opinion, I think that the models surrounding these latent spaces are quite neat: they hint at the idea that there exists a fundamental discrepancy between what is overtly shown and how these patterns come to be."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://sunnyson.dev/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Monocle - Reverse Graph Embeddings",
      "item": "https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Monocle - Reverse Graph Embeddings",
  "name": "Monocle - Reverse Graph Embeddings",
  "description": "Everything is Not What They Seem More than just a throwback to Wizards of Waverly Place, this statement is fundamental to the way our world operates. Specifically, it hints at the existance of a hidden dimension, a latent space that exists beneath the surface of our reality. And in my opinion, I think that the models surrounding these latent spaces are quite neat: they hint at the idea that there exists a fundamental discrepancy between what is overtly shown and how these patterns come to be.",
  "keywords": [
    "monocle", "latent-model", "dimensionality-reduction", "reverse-graph-embeddings"
  ],
  "articleBody": "Everything is Not What They Seem More than just a throwback to Wizards of Waverly Place, this statement is fundamental to the way our world operates. Specifically, it hints at the existance of a hidden dimension, a latent space that exists beneath the surface of our reality. And in my opinion, I think that the models surrounding these latent spaces are quite neat: they hint at the idea that there exists a fundamental discrepancy between what is overtly shown and how these patterns come to be.\nThis concept shines brightly in the intricate systems governing gene expression. Here, phenotypes (the observable traits of an organism, i.e. the fiery red versus raven black hair in humans, or the varying petal lengths among different iris species) aren’t always directly correlated with genotypes (the specific sequence of ATCG: Adenine, Thymine, Cytosine, and Guanine in DNA).\nIn the diverse landscape of the medical field, the phenotypes of a particular organism can sometimes seem cryptic when viewed in the context of the genes involved. Not to mention, there is much translational difference in the process governing what genes are expressed, how the expression arises, and when. To uncover the mechanisms that govern these processes, we would need to dive into the unknown, and as it turns out, the experimental observations of cellular expression (in terms of the genes active in each cell) exist in a lower dimensional latent space (Trapnell et al.). This is in stark contrast to the more complex, higher dimensional variables that we might initially expect.\nGene expression is not unlike a magic trick. What we see on the surface is just the tip of the iceberg, with what we think is a world of complexity hidden beneath, but is actually simpler than what we might actually expect. But more importantly, like a good magic trick, it’s this hidden complexity that makes it all the more fascinating.\nThe Analogy of the Ball: Unraveling Reverse Graphs To demystify the complexities of reverse graph embeddings, let’s employ a straightforward yet effective analogy. Picture someone launching a ball into the air. You’re armed with a camera, tracking the ball’s trajectory from a third-person perspective off the $z$-axis. You capture the journey in a series of snapshots, snap, snap, snap, snap, … , SNAP. Once the film is developed, you’re left with a series of distinct photos.\nThese snapshots serve as frozen moments in time, each providing a unique perspective on the ball’s position relative to its surroundings. The photos might resemble something like this:\nFig. 1. Five distinct photos capturing the ball's trajectory through the air While you’ve captured a total of five images, this number can be generalized to N, allowing for as many or as few snapshots as desired. Each snapshot provides an accurate representation of the ball’s location at that specific moment. A more comprehensive view emerges when we zoom out to consider the bigger picture:\nFig. 2. Perspective from the z-axis of a ball being thrown into the air, with each ball representing a snapshot in time The above image (Fig. 2) is a continuous representation of the ball’s trajectory, showing it moving through space and time.\nFor a more holistic understanding, we can juxtapose the discrete snapshots with the continuous trajectory of the ball. The image below illustrates this, overlaying the snapshots at each time point:\nFig. 3. The combined image of both the snapshots and the continuous trajectory To infuse some mathematical intuition into these images, we can use a simple kinematics equation representing a point-like object (our ball):\n$$ \\begin{aligned} x(t) \u0026 = \\frac{1}{2} a_it^2 + v_it + x_i, \\\\ \u0026 t \\in \\mathbb{R}; \\ x_i, v_i, a_i \\in \\mathbb{R}^3 \\end{aligned} $$\nHere, $x_i$ is the initial position of the ball, $v_i$ is the initial velocity, and $a_i$ is the initial acceleration, with all variables existing in a hypothetical $n$-dimensional space (our world). $t$ represents the time elapsed since the ball was tossed. $x(t)$ is the height of the ball at any timepoint $t$.\nFor a moment, let’s pretend we know nothing about the continuous path that the ball takes, what we consider to be the closed-form solution, or the above equation that represents the ball’s position ($x_i$). Instead, let’s ponder: how can we piece together the ball’s trajectory from the discrete images shown in Fig. 1?\nDrawing inspiration from the combined discrete/continuous image (Fig. 3), which bridges the time-agnostic (Fig. 1) and time-aware (Fig. 2) figures, we notice that the most significant difference in each discrete image is the background (with the ball serving as a constant reference).\nCan we now quantify this difference of background? The answer lies in the change of the position of the ball, which alters the background from the perspective of a third-party observer. This insight is precisely the foundation of dimensionality reduction through graph learning: to find the path of the ball (the closed equation) that best describes all the images of the ball, given its position (the discrete images) at different timepoints when we know nothing about the realities of our world, specifically how we live in a $3$d world or the laws of physics (kinematics) that need to be followed.\nThis analogy serves as a bridge between a simpler, physical representation and the complex biological processes it aims to explain. By generalizing, we can align this analogy with what the algorithm was designed to represent: cells and their expressed genes. In this context, gene expression can be likened to the snapshots of individual cells, and the latent distribution of possible outcomes (moderated by time) can be viewed as the continuous trajectory. This continuous trajectory (which will be completed in a following post) exists in a lower dimension than the number of dimensions for the expressed phenotype (the total number of expressed genes). But how do we determine this optimal lower dimension to reduce our dataset to?\nRiding the Waves and Webs: A Dive into Genotypes and Phenotypes The previous analogy, while effective as a broad concept, is somewhat deterministic. Given the parameters $x_i$, $v_i$, and $a_i$, the height $x(t)$ can be calculated with precision. However, this approach doesn’t quite capture the randomness inherent in our genotype vs. phenotype conundrum.\nTo clarify, let’s extend our analogy by drawing inspiration from nature, specifically waves and water:\nFig. 4. Water waves with refractions as seen from above Beyond its aesthetic appeal, this image carries a metaphorical significance. The randomness depicted in the waves, demonstrated by the unpredictable splash and splatter of the water, alongside the seemingly continuous fabric of the water’s surface, more accurately represents the nature of gene expression (phenotype) than a simple ball toss. Even better, it presents as a surface rather than a point-like, individual object able to describe a whole spectra of quantities.\nSo, where does the genotype fit into this picture? Think of the genotype as a spider web, subtly fluctuating under the force of the wind, yet never so erratic as to lose its structural integrity. In many ways, it’s more stable than the phenotype, thanks to the codified nature of DNA. However, it’s also simpler, given the relatively minor variation between one instance of genotype and another (as seen when comparing gene expression in cells).\nFig. 5. The simplicity of a spider web in comparison to water waves With a deeper understanding of this metaphor, we’re now ready to introduce mathematical concepts and numbers into our discussion.\nThe Peaks and Valleys: A Mathematical Perspective As previously discussed, waves can represent the high-dimensional output of genes that are expressed as a phenotype.\nFig. 6. Our mathematical model of high-dimensional waves In this mathematical model, we assume there’s a method to sequentially number all states of the expressed genes (phenotypes) in the high-dimensional space, denoted as $\\mathcal{X}={\\mathbf{x}_1,\\mathbf{x}_2,…,\\mathbf{x}_N}$, where $N \\in \\mathbb{I}$ is the total number of cells. Essentially, this is a comprehensive list of the dimensions associated with each cell (i.e., the vector of genes expressed per cell) for every cell in the sampled tissue.\nFig. 7. Our mathematical model of low-dimensional webs In the lower-dimensional space, we define the variables associated with the diversity of gene expressions (genotypes) across individual cells as $\\mathcal{Z}={\\mathbf{z}_1, \\mathbf{z}_2,…,\\mathbf{z}_N}$, where $N \\in \\mathbb{I}$ is the total number of cells. However, in this context, the representation of each variable is less clear. They consist of latent (unseen) nodes, each representing a cell’s true phenotype-generating distribution. In simpler terms, these latent variables are not directly observed but are inferred from the observed gene expressions. They serve as a representation of the underlying biological states or properties of each cell.\nCharting the Course: A Map(ping) to Guide Us As we stand on the precipice of defining the structured relationship between the web and the wave, we’re essentially trying to distill a high-dimensional output into a low-dimensional insight. The first step in this journey is to visualize how we define this low-dimensional web that we’re aiming to describe. In other words, we need to identify the abstract qualities that our web should possess.\nIf we take a gander at Fig. 7, a few initial qualities jump out at us:\nThe circular components that house each of the cells, which we’ll affectionately dub as nodes henceforth. The lines that serve as the social glue connecting these circles, playing the role of edges. The culmination of these elements, which we’ll refer to as a graph. To put on our math hats for a moment, we can now express that the Graph $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E})$ comprises a set of vertices $\\mathcal{V}={\\mathcal{V}_1, \\mathcal{V}_2,…,\\mathcal{V}_N}$ and a set of weighted, undirected edges $\\mathcal{E}$. This is a far cry from unweighted and directed edges. The former lacks a mechanism to represent the numerical degree of connection between any two vertices, while the latter implies a sense of directionality or pointing from one node vertex to another.\nReverse Graph Embeddings: Setting Sail with Our Captain Now, let’s dive into the actual process of mapping from the higher dimension to the low. For this, we’ll employ a nifty tool known as Reverse Graph Embeddings. Here’s the equation that makes the magic happen:\n$$ \\min_{\\mathcal{G} \\in G_b}{} \\min_{f_{\\mathcal{G}} \\in \\mathcal{F}}{} \\min_{Z}{} \\sum_{(V_i, V_j) \\in \\mathcal{E}} b_{i,j} || f_G (\\mathbf{z}_i) - f_G (\\mathbf{z}_j) ||^2 $$\nTo break it down, this equation is the hero of our adventure that helps us transform (defeat) a complex, high-dimensional data structure into a simpler, low-dimensional representation. It’s like turning a spaghetti monster into a neat, orderly pasta. And who doesn’t love a good pasta?\nIt does so through a yet-to-be named function, a mapping between the high dimensional expressions and the low dimensional representation of the genes in question (hint, we will name it in another post!)\nFrustrating Minimums (Argghh Mins) Let’s delve into the intricacies of a triple nested optimization problem, a prevalent construct in machine learning. This complex structure, akin to a Russian nesting doll, involves the pursuit of optimal model parameters that minimize a specific loss function.\nWhat is a loss function? In essence, it’s a measure of how well a model can predict the expected outcome. The lower the loss, the better the model. In our case, the loss function is the summation in the equation, which we’ll explore in greater detail later on.\nDespite its initial complexity, we will systematically dissect all of the annoying minimums for a more in-depth look at what the model selects for.\n$$ \\min_{\\mathcal{G} \\in G_b}{} \\min_{f_{\\mathcal{G}} \\in \\mathcal{F}}{} \\min_{Z}{} $$\nThe target objective (what we are minimizing for) comes in the form of a triple nested optimization problem, which is common in machine learning when we’re trying to find the best model parameters that minimize a certain loss function. Let’s start from the inside and work our way out:\nThe First $\\min$ selects the optimal graph $\\mathcal{G}$:\n$$ \\min_{\\mathcal{G} \\in \\mathcal{G}_b} $$\nAt the core of Reverse Graph Embeddings lies a graph $\\mathcal{G}$ that captures the relationships between data points. However, not all graphs are created equal. The first $\\min$ allows us to explore different graphs within the set $\\mathcal{G}_b$ and identify the one that best represents the underlying structure of the data.\nThe Second $\\min$ optimizes the embedding function $\\mathcal{f}_{\\mathcal{G}}$:\n$$ \\min_{\\mathcal{f}_{\\mathcal{G}} \\in \\mathcal{F}} $$\nOnce we’ve identified the optimal graph $\\mathcal{G}$, the next step is to determine the most suitable embedding function, denoted as $\\mathcal{f}_{\\mathcal{G}}$. This function is responsible for mapping data points from a higher-dimensional space to a lower-dimensional one, akin to representing a 3D globe on a 2D map. The challenge lies in preserving the relationships between data points and the graph’s structure during this transformation.\nThe second $\\min$ guides us through the set $\\mathcal{F}$ of potential embedding functions, helping us find the optimal $\\mathcal{f}_{\\mathcal{G}}$ that simplifies our data while maintaining its inherent structure and relationships. This balancing act is essential for effective machine learning.\nThe Third $\\min$ finds the Optimal Embeddings $Z$:\n$$ \\min_{Z} $$\nWith the graph $\\mathcal{G}$ and the embedding function $\\mathcal{f}_{\\mathcal{G}}$ in place, the third $\\min$ seeks to identify the optimal low-dimensional embeddings $Z$. These embeddings are representations of the data points in the lower-dimensional space that minimize the loss function defined by the summation in the equation.\n(Simplified) Loss Functions Now, we get to the simplified loss function, or the part of the equation stated as:\n$$ \\sum_{(V_i, V_j) \\in \\mathcal{E}} b_{i,j} || f_G (\\mathbf{z}_i) - f_G (\\mathbf{z}_j) ||^2 $$\nThe objective of the loss function is to measure the similarity or dissimilarity between two data points in the graph $\\mathcal{G}$. As we traverse the graph’s edges represented by $(V_i, V_j)$ in the set $\\mathcal{E}$, the loss function computes the difference between the embeddings of these connected data points.\nGraph-Based Weights:\nThe loss function incorporates the notion of graph-based weights $b_{i,j}$. These weights allow us to assign different levels of importance to the edges connecting data points within the graph $\\mathcal{G}$. By introducing these weights, we can emphasize or de-emphasize certain relationships, depending on their significance in the overall data representation.\nFig. 8. A brief overview of how edges and nodes interract In the above (Fig. 8), we can define $\\varepsilon_{i,j}$ as the edge connecting two nodes $V_i$ and $V_j$. The weight $b_{i,j}$ is a measure of the importance of this edge in the overall graph $\\mathcal{G}$. The higher the weight, the more significant the edge, and vice versa. This concept is similar to the notion of edge strength in social networks, where the weight of an edge represents the strength of the relationship between any two individuals.\nOptimizing Low-Dimensional Embeddings:\nThe central aim of Reverse Graph Embeddings is to minimize the loss function. As the triple nested optimization progresses, the graph $\\mathcal{G}$ and the embedding function $\\mathcal{f}_{\\mathcal{G}}$ are fine-tuned to achieve the optimal low-dimensional embeddings $Z$. These embeddings represent the data in a compact and meaningful manner in the lower-dimensional space.\nPreserving Graph Structure:\nThrough the process of minimizing the loss function, the algorithm ensures that the relationships between data points observed in the original graph $\\mathcal{G}$ are maintained in the lower-dimensional space. This preservation of graph structure is essential for extracting meaningful insights and knowledge from the data.\nComplete Loss Function and Visualization This is a simplified version of the complete Reverse Graph Embedding, only considering graph structures in the latent (genotype) state, but not the observed phenotypes within the optimization parameters. To find a way to connect the high and low dimensions, RGE both ensures that\nThe image under the function $\\mathcal{f}_{\\mathcal{G}}$ (points in the high-dimensional space as a function of the low-dimensional space) are close to one another as we described previously, multiplied by the weigbht of the associated edge $\\mathcal{E}$, but save for the $\\lambda$ hyperparameter which will be explained later on: $$ \\frac{\\lambda}{2} \\sum_{(V_i, V_j) \\in \\mathcal{E}} b_{i,j} || f_G (\\mathbf{z}_i) - f_G (\\mathbf{z}_j) ||^2 $$\nAn addended portion states that points which are neighbors on the low-dimensional principal graph are also “neighbors” in the input dimension, meaning for a given $\\mathbf{z}_i$ (where $i$ is the index of the vector associated with a specific cell), the estimated phenotypes expressed must also be similar to the real phenotypes expressed by the cell in the high-dimension: $$ \\sum_{i=1}^{N} || \\mathbf{x}_i - f_G (\\mathbf{z}_i) ||^2 $$\nResulting in the combined equation:\n$$ \\min_{\\mathcal{G} \\in G_b}{} \\min_{f_{\\mathcal{G}} \\in \\mathcal{F}}{} \\min_{Z}{} \\sum_{i=1}^{N} || \\mathbf{x}_i - f_G (\\mathbf{z}_i) ||^2 $$\n$$ \\text{+} \\frac{\\lambda}{2} \\sum_{(V_i, V_j) \\in \\mathcal{E}} b_{i,j} || f_G (\\mathbf{z}_i) - f_G (\\mathbf{z}_j) ||^2 $$\nWhich when held together, represents the optimization constraints required to find and reduce a set of vectors associated with the high number of dimensions for each cell to an optimal number of lower dimensions for each cell. The below figure best summarizes the math behind what is taking place:\nFig. 9. Our completed, mathematically sound model of webs to waves The image essentially illustrates that there is a function $\\mathcal{f}_{\\mathcal{G}}$ that maps lower-dimensional vectors in $\\mathbb{R}^G$ (meaning they have $G$ number of dimensions) to a higher-dimensional space $\\mathbb{R}^P$ (meaning they have $P$ number of expressed genes). The lower-dimensional graph, represented as a simpler web, is mapped to the higher-dimensional manifold (a smooth collection of points), depicted as a wave. This is a characteristic process in Monocle, a tool used for single-cell RNA-seq analysis.\nAnd where does this lead in the end? In all honesty, the battle is only half over. The first half we just completed deals with creating the road that we will decide on how to drive down, finding the “optimal” (most likely) graph of latent variables that generates the phenotypes captured by whatever genes were sequenced. The next half finally buys a car and sets the direction of travel, lending to understanding a pseudo-temporal ordering of expression and how similarity in the lower-dimensional means for general similarity in time as well.\nThis will take us back, to revisit and understand further the analogy of the ball toss and how time can be used as a red-thread of fate to tie together the disparate destinies of each individual cell.\nThe Conclusion? In this post, we’ve journeyed through concepts surrounding latent models, gene expression, and the intricate systems that govern these phenomena. We’ve used analogies that (I hope) would make even the most hardened scientist crack a smile, and visual aids that might make my high school art teacher finally be proud of me, all to help us understand these complex concepts.\nWe’ve delved into the mathematical ocean of Reverse Graph Embeddings, a powerful tool for dimensionality reduction. We’ve seen how these techniques can illuminate the relationship between genotypes and phenotypes, and how they can map high-dimensional data onto a lower-dimensional space while preserving the underlying structure. It’s like fitting an elephant into a phone booth, but without violating any animal rights!\nYou probably can't fit an elephant in a phone booth like this, so this elephant was probably dimensionally reduced! We’ve also explored how these techniques help us identify the optimal graph, the most suitable embedding function, and the best low-dimensional embeddings that represent the data. We’ve looked at the role of graph-based weights in assigning different levels of importance to the edges connecting data points within the graph.\nAnd let’s not forget how the algorithm ensures that the relationships between data points observed in the original graph are maintained in the lower-dimensional space, like moving across the country but still finding the bandwith to keep in touch with your old friends.\nKeeping up with friends is important when you are somewhere different The main paper on Monocle (Qiu et al.), takes this concept even further. It explores how trees are employed to create a pseudo-temporal map of cells in various states of division, a journey which I will save for my next post. Yes, I’m leaving you on a cliffhanger and sorry, not sorry.\nThank you for reading! And, please look forward to the next post that will set sail for high-dimensional oceans once more, when we cover the rest of graph learning for monocle. I promise it will be worth the wait!\nWho doesn't love the imagery of a cute pirate captain sailing the high seas? Citation Cited as:\nSon, Sunny. (Jul 2023). \"Monocle - Reverse Graph Embeddings\". Sunny's Notes. https://sunnyson.dev/notes/2023/07/monocle-reverse-graph-embeddings/. Or, in BibTeX format:\n@article{son-2023-monocle-pt1, title = \"Monocle - Reverse Graph Embeddings\", author = \"Son, Sunny\", journal = \"sunnyson.dev\", year = \"2023\", month = \"July\", url = \"https://sunnyson.dev/notes/2023/07/monocle-reverse-graph-embeddings/\" } References [1] Trapnell, C. et al. “The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells.\" Nature Biotechnology 2014\n[2] Qiu, Xiaojie. et al. “Reversed graph embedding resolves complex single-cell trajectories\" Nature Methods 2017\n",
  "wordCount" : "3419",
  "inLanguage": "en",
  "image":"https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/images/dalle-white-ball.jpg","datePublished": "2023-07-23T07:00:00-04:00",
  "dateModified": "2023-07-23T07:00:00-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sunnyson.dev/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>

        
        <button id="menu-trigger" aria-haspopup="menu" aria-label="Menu Button">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
                <line x1="3" y1="12" x2="21" y2="12"></line>
                <line x1="3" y1="6" x2="21" y2="6"></line>
                <line x1="3" y1="18" x2="21" y2="18"></line>
            </svg>
        </button>
        <ul class="menu hidden">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Monocle - Reverse Graph Embeddings
    </h1>
    <div class="post-meta"><span title='2023-07-23 07:00:00 -0400 -0400'>July 23, 2023</span>

</div>
  </header> 
<figure class="entry-cover">
        <img loading="lazy" srcset="https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/images/dalle-white-ball_huad71b4dc33a3897bc35f0c99621ff568_48396_360x0_resize_q75_box.jpg 360w ,https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/images/dalle-white-ball_huad71b4dc33a3897bc35f0c99621ff568_48396_480x0_resize_q75_box.jpg 480w ,https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/images/dalle-white-ball_huad71b4dc33a3897bc35f0c99621ff568_48396_720x0_resize_q75_box.jpg 720w ,https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/images/dalle-white-ball.jpg 972w" 
            sizes="(min-width: 768px) 720px, 100vw" src="https://sunnyson.dev/notes/monocle-reverse-graph-embeddings/images/dalle-white-ball.jpg" alt="" 
            width="972" height="972">
        
</figure>
<div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#everything-is-not-what-they-seem" aria-label="Everything is Not What They Seem">Everything is Not What They Seem</a></li>
                <li>
                    <a href="#the-analogy-of-the-ball-unraveling-reverse-graphs" aria-label="The Analogy of the Ball: Unraveling Reverse Graphs">The Analogy of the Ball: Unraveling Reverse Graphs</a></li>
                <li>
                    <a href="#riding-the-waves-and-webs-a-dive-into-genotypes-and-phenotypes" aria-label="Riding the Waves and Webs: A Dive into Genotypes and Phenotypes">Riding the Waves and Webs: A Dive into Genotypes and Phenotypes</a></li>
                <li>
                    <a href="#the-peaks-and-valleys-a-mathematical-perspective" aria-label="The Peaks and Valleys: A Mathematical Perspective">The Peaks and Valleys: A Mathematical Perspective</a><ul>
                        
                <li>
                    <a href="#charting-the-course-a-mapping-to-guide-us" aria-label="Charting the Course: A Map(ping) to Guide Us">Charting the Course: A Map(ping) to Guide Us</a></li></ul>
                </li>
                <li>
                    <a href="#reverse-graph-embeddings-setting-sail-with-our-captain" aria-label="Reverse Graph Embeddings: Setting Sail with Our Captain">Reverse Graph Embeddings: Setting Sail with Our Captain</a><ul>
                        
                <li>
                    <a href="#frustrating-minimums-argghh-mins" aria-label="Frustrating Minimums (Argghh Mins)">Frustrating Minimums (Argghh Mins)</a></li>
                <li>
                    <a href="#simplified-loss-functions" aria-label="(Simplified) Loss Functions">(Simplified) Loss Functions</a></li>
                <li>
                    <a href="#complete-loss-function-and-visualization" aria-label="Complete Loss Function and Visualization">Complete Loss Function and Visualization</a></li></ul>
                </li>
                <li>
                    <a href="#the-conclusion" aria-label="The Conclusion?">The Conclusion?</a></li>
                <li>
                    <a href="#citation" aria-label="Citation">Citation</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>





  <div class="post-content"><h2 id="everything-is-not-what-they-seem">Everything is Not What They Seem<a hidden class="anchor" aria-hidden="true" href="#everything-is-not-what-they-seem">#</a></h2>
<p>More than just a throwback to Wizards of Waverly Place, this statement is fundamental to the way our world operates. Specifically, it hints at the existance of a hidden dimension, a <em>latent</em> space that exists beneath the surface of our reality. And in my opinion, I think that the models surrounding these latent spaces are quite neat: they hint at the idea that there exists a fundamental discrepancy between <em>what</em> is overtly shown and <em>how</em> these patterns come to be.</p>
<p>This concept shines brightly in the intricate systems governing gene expression. Here, phenotypes (the observable traits of an organism, i.e. the fiery red versus raven black hair in humans, or the varying petal lengths among different iris species) aren&rsquo;t always directly correlated with genotypes (the specific sequence of <em>ATCG</em>: <em>Adenine</em>, <em>Thymine</em>, <em>Cytosine</em>, and <em>Guanine</em> in DNA).</p>
<p>In the diverse landscape of the medical field, the phenotypes of a particular organism can sometimes seem cryptic when viewed in the context of the genes involved. Not to mention, there is much <em>translational</em> difference in the process governing <em>what</em> genes are expressed, <em>how</em> the expression arises, and <em>when</em>. To uncover the mechanisms that govern these processes, we would need to dive into the unknown, and as it turns out, the experimental observations of cellular expression (in terms of the genes active in each cell) exist in a lower dimensional <em>latent</em> space (<a href="https://www.nature.com/articles/nbt.2859">Trapnell et al.</a>). This is in stark contrast to the more complex, higher dimensional variables that we might initially expect.</p>
<p>Gene expression is not unlike a magic trick. What we see on the surface is just the tip of the iceberg, with what we think is a world of complexity hidden beneath, but is actually simpler than what we might actually expect. But more importantly, like a good magic trick, it&rsquo;s this hidden complexity that makes it all the more fascinating.</p>
<h2 id="the-analogy-of-the-ball-unraveling-reverse-graphs">The Analogy of the Ball: Unraveling Reverse Graphs<a hidden class="anchor" aria-hidden="true" href="#the-analogy-of-the-ball-unraveling-reverse-graphs">#</a></h2>
<p>To demystify the complexities of reverse graph embeddings, let&rsquo;s employ a straightforward yet effective analogy. Picture someone launching a ball into the air. You&rsquo;re armed with a camera, tracking the ball&rsquo;s trajectory from a third-person perspective off the $z$-axis. You capture the journey in a series of snapshots, <em>snap</em>, <em>snap</em>, <em>snap</em>, <em>snap</em>, &hellip; , <em>SNAP</em>. Once the film is developed, you&rsquo;re left with a series of distinct photos.</p>
<p>These snapshots serve as frozen moments in time, each providing a unique perspective on the ball&rsquo;s position relative to its surroundings. The photos might resemble something like this:</p>
<figure id="fig1">
    <img src="images/fig-02.png" alt="Fig. 1">
    <figcaption align="center"><i>Fig. 1</i>. Five distinct photos capturing the ball's trajectory through the air </figcaption>
</figure>
<p>While you&rsquo;ve captured a total of five images, this number can be generalized to <em>N</em>, allowing for as many or as few snapshots as desired. Each snapshot provides an accurate representation of the ball&rsquo;s location at that specific moment. A more comprehensive view emerges when we zoom out to consider the bigger picture:</p>
<figure style="text-align: center;" id="fig2">
    <img src="images/fig-01.png" width="70%" alt="Fig. 2" style="display: inline-block;">
    <figcaption align="center"><i>Fig. 2</i>. Perspective from the z-axis of a ball being thrown into the air, with each ball representing a snapshot in time</figcaption>
</figure>
<p>The above image (<a href="#fig2">Fig. 2</a>) is a <em>continuous</em> representation of the ball&rsquo;s trajectory, showing it moving through space and time.</p>
<p>For a more holistic understanding, we can juxtapose the <em>discrete</em> snapshots with the <em>continuous</em> trajectory of the ball. The image below illustrates this, overlaying the snapshots at each time point:</p>
<figure style="text-align: center;" id="fig3">
    <img src="images/fig-03.png" width="70%" alt="Fig. 3" style="display: inline-block;">
    <figcaption align="center"><i>Fig. 3</i>. The combined image of both the snapshots and the continuous trajectory</figcaption>
</figure>
<p>To infuse some mathematical intuition into these images, we can use a simple kinematics equation representing a point-like object (our ball):</p>
<p>$$
\begin{aligned}
x(t) &amp; = \frac{1}{2} a_it^2 + v_it + x_i, \\
&amp; t \in \mathbb{R}; \ x_i, v_i, a_i \in \mathbb{R}^3
\end{aligned}
$$</p>
<p>Here, $x_i$ is the initial position of the ball, $v_i$ is the initial velocity, and $a_i$ is the initial acceleration, with all variables existing in a <em>hypothetical</em> $n$-dimensional space (our world). $t$ represents the time elapsed since the ball was tossed. $x(t)$ is the height of the ball at any timepoint $t$.</p>
<p>For a moment, let&rsquo;s pretend we know nothing about the <em>continuous</em> path that the ball takes, what we consider to be the <a href="https://en.wikipedia.org/wiki/Closed-form_expression"><em>closed-form solution</em></a>, or the above equation that represents the ball&rsquo;s position ($x_i$). Instead, let&rsquo;s ponder: how can we piece together the ball&rsquo;s trajectory from the discrete images shown in <a href="#fig1">Fig. 1</a>?</p>
<p>Drawing inspiration from the combined discrete/continuous image (<a href="#fig3">Fig. 3</a>), which bridges the time-agnostic (<a href="#fig1">Fig. 1</a>) and time-aware (<a href="#fig2">Fig. 2</a>) figures, we notice that the most significant difference in each discrete image is the background (with the <em>ball</em> serving as a constant reference).</p>
<p>Can we now quantify this difference of background? The answer lies in the change of the <em>position</em> of the ball, which alters the background from the perspective of a third-party observer. This insight is precisely the foundation of dimensionality reduction through graph learning: to find the <em>path</em> of the ball (the closed equation) that best describes all the images of the ball, given its position (the discrete images) at different timepoints when we know nothing about the realities of our world, specifically how we live in a $3$d world or the laws of physics (kinematics) that need to be followed.</p>
<p>This analogy serves as a bridge between a simpler, physical representation and the complex biological processes it aims to explain. By generalizing, we can align this analogy with what the algorithm was designed to represent: cells and their expressed genes. In this context, gene expression can be likened to the snapshots of individual cells, and the latent distribution of <em>possible</em> outcomes (moderated by time) can be viewed as the continuous trajectory. This continuous trajectory (which will be completed in a following post) exists in a lower dimension than the number of dimensions for the expressed phenotype (the total number of expressed genes). But how do we determine this optimal lower dimension to reduce our dataset to?</p>
<h2 id="riding-the-waves-and-webs-a-dive-into-genotypes-and-phenotypes">Riding the Waves and Webs: A Dive into Genotypes and Phenotypes<a hidden class="anchor" aria-hidden="true" href="#riding-the-waves-and-webs-a-dive-into-genotypes-and-phenotypes">#</a></h2>
<p>The previous analogy, while effective as a broad concept, is somewhat <em>deterministic</em>. Given the parameters $x_i$, $v_i$, and $a_i$, the height $x(t)$ can be calculated with precision. However, this approach doesn&rsquo;t quite capture the randomness inherent in our genotype vs. phenotype conundrum.</p>
<p>To clarify, let&rsquo;s extend our analogy by drawing inspiration from nature, specifically waves and water:</p>
<figure style="text-align: center;" id="fig4">
    <img src="images/fig-04.gif" width="70%" alt="Fig. 4" style="display: inline-block;">
    <figcaption align="center"><i>Fig. 4</i>. Water waves with refractions as seen from above</figcaption>
</figure>
<p>Beyond its aesthetic appeal, this image carries a metaphorical significance. The randomness depicted in the waves, demonstrated by the unpredictable <em>splash and splatter</em> of the water, alongside the seemingly continuous <em>fabric</em> of the water&rsquo;s surface, more accurately represents the nature of gene expression (phenotype) than a simple ball toss. Even better, it presents as a <em>surface</em> rather than a point-like, individual object able to describe a whole spectra of quantities.</p>
<p>So, where does the genotype fit into this picture? Think of the genotype as a spider web, subtly fluctuating under the force of the wind, yet never so erratic as to lose its structural integrity. In many ways, it&rsquo;s more stable than the phenotype, thanks to the codified nature of DNA. However, it&rsquo;s also <em>simpler</em>, given the relatively minor variation between one instance of genotype and another (as seen when comparing gene expression in cells).</p>
<figure style="text-align: center;" id="fig5">
    <img src="images/fig-05.gif" width="70%" alt="Fig. 5" style="display: inline-block;">
    <figcaption align="center"><i>Fig. 5</i>. The simplicity of a spider web in comparison to water waves</figcaption>
</figure>
<p>With a deeper understanding of this metaphor, we&rsquo;re now ready to introduce mathematical concepts and numbers into our discussion.</p>
<h2 id="the-peaks-and-valleys-a-mathematical-perspective">The Peaks and Valleys: A Mathematical Perspective<a hidden class="anchor" aria-hidden="true" href="#the-peaks-and-valleys-a-mathematical-perspective">#</a></h2>
<p>As previously discussed, waves can represent the high-dimensional output of genes that are expressed as a phenotype.</p>
<figure style="text-align: center;" id="fig6">
    <img src="images/fig-06.png" width="70%" alt="Fig. 6" style="display: inline-block;">
    <figcaption align="center"><i>Fig. 6</i>. Our mathematical model of high-dimensional waves</figcaption>
</figure>
<p>In this mathematical model, we assume there&rsquo;s a method to sequentially number all states of the expressed genes (phenotypes) in the high-dimensional space, denoted as $\mathcal{X}={\mathbf{x}_1,\mathbf{x}_2,&hellip;,\mathbf{x}_N}$, where $N \in \mathbb{I}$ is the total number of cells. Essentially, this is a comprehensive list of the dimensions associated with each cell (i.e., the vector of genes expressed per cell) for every cell in the sampled tissue.</p>
<figure style="text-align: center;" id="fig7">
    <img src="images/fig-07.png" width="70%" alt="Fig. 7" style="display: inline-block;">
    <figcaption align="center"><i>Fig. 7</i>. Our mathematical model of low-dimensional webs</figcaption>
</figure>
<p>In the lower-dimensional space, we define the variables associated with the diversity of gene expressions (genotypes) across individual cells as $\mathcal{Z}={\mathbf{z}_1, \mathbf{z}_2,&hellip;,\mathbf{z}_N}$, where $N \in \mathbb{I}$ is the total number of cells. However, in this context, the representation of each variable is less clear. They consist of <em>latent</em> (unseen) nodes, each representing a cell&rsquo;s true phenotype-generating distribution. In simpler terms, these latent variables are not directly observed but are inferred from the observed gene expressions. They serve as a representation of the underlying biological states or properties of each cell.</p>
<h3 id="charting-the-course-a-mapping-to-guide-us">Charting the Course: A Map(ping) to Guide Us<a hidden class="anchor" aria-hidden="true" href="#charting-the-course-a-mapping-to-guide-us">#</a></h3>
<p>As we stand on the precipice of defining the structured relationship between the web and the wave, we&rsquo;re essentially trying to distill a high-dimensional output into a low-dimensional insight. The first step in this journey is to visualize <em>how</em> we define this low-dimensional web that we&rsquo;re aiming to describe. In other words, we need to identify the abstract qualities that our web should possess.</p>
<p>If we take a gander at <a href="#fig7">Fig. 7</a>, a few initial qualities jump out at us:</p>
<ol>
<li>The circular components that house each of the cells, which we&rsquo;ll affectionately dub as <em>nodes</em> henceforth.</li>
<li>The lines that serve as the social glue connecting these circles, playing the role of <em>edges</em>.</li>
<li>The culmination of these elements, which we&rsquo;ll refer to as a <em>graph</em>.</li>
</ol>
<p>To put on our math hats for a moment, we can now express that the Graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ comprises a set of vertices $\mathcal{V}={\mathcal{V}_1, \mathcal{V}_2,&hellip;,\mathcal{V}_N}$ and a set of weighted, undirected edges $\mathcal{E}$. This is a far cry from unweighted and directed edges. The former lacks a mechanism to represent the numerical degree of connection between any two vertices, while the latter implies a sense of directionality or pointing from one node vertex to another.</p>
<h2 id="reverse-graph-embeddings-setting-sail-with-our-captain">Reverse Graph Embeddings: Setting Sail with Our Captain<a hidden class="anchor" aria-hidden="true" href="#reverse-graph-embeddings-setting-sail-with-our-captain">#</a></h2>
<p>Now, let&rsquo;s dive into the actual process of mapping from the higher dimension to the low. For this, we&rsquo;ll employ a nifty tool known as Reverse Graph Embeddings. Here&rsquo;s the equation that makes the magic happen:</p>
<p>$$
\min_{\mathcal{G} \in G_b}{} \min_{f_{\mathcal{G}} \in \mathcal{F}}{} \min_{Z}{} \sum_{(V_i, V_j) \in \mathcal{E}} b_{i,j}  || f_G (\mathbf{z}_i) - f_G (\mathbf{z}_j) ||^2
$$</p>
<p>To break it down, this equation is the hero of our adventure that helps us transform (defeat) a complex, high-dimensional data structure into a simpler, low-dimensional representation. It&rsquo;s like turning a spaghetti monster into a neat, orderly pasta. And who doesn&rsquo;t love a good pasta?</p>
<p>It does so through a yet-to-be named function, a mapping between the high dimensional expressions and the low dimensional representation of the genes in question (hint, we will name it in another post!)</p>
<h3 id="frustrating-minimums-argghh-mins">Frustrating Minimums (Argghh Mins)<a hidden class="anchor" aria-hidden="true" href="#frustrating-minimums-argghh-mins">#</a></h3>
<p>Let&rsquo;s delve into the intricacies of a triple nested optimization problem, a prevalent construct in machine learning. This complex structure, akin to a Russian nesting doll, involves the pursuit of optimal model parameters that minimize a specific loss function.</p>
<p>What <em>is</em> a loss function? In essence, it&rsquo;s a measure of how well a model can predict the expected outcome. The lower the loss, the better the model. In our case, the loss function is the summation in the equation, which we&rsquo;ll explore in greater detail later on.</p>
<p>Despite its initial complexity, we will systematically dissect all of the annoying minimums for a more in-depth look at what the model selects for.</p>
<p>$$
\min_{\mathcal{G} \in G_b}{} \min_{f_{\mathcal{G}} \in \mathcal{F}}{} \min_{Z}{}
$$</p>
<p>The target objective (what we are minimizing for) comes in the form of a triple nested optimization problem, which is common in machine learning when we&rsquo;re trying to find the best model parameters that minimize a certain loss function. Let&rsquo;s start from the inside and work our way out:</p>
<ol>
<li>
<p>The First $\min$ selects the optimal graph $\mathcal{G}$:</p>
<p>$$
\min_{\mathcal{G} \in \mathcal{G}_b}
$$</p>
<p>At the core of Reverse Graph Embeddings lies a graph $\mathcal{G}$ that captures the relationships between data points. However, not all graphs are created equal. The first $\min$ allows us to explore different graphs within the set $\mathcal{G}_b$ and identify the one that best represents the underlying structure of the data.</p>
</li>
<li>
<p>The Second $\min$ optimizes the embedding function $\mathcal{f}_{\mathcal{G}}$:</p>
<p>$$
\min_{\mathcal{f}_{\mathcal{G}} \in \mathcal{F}}
$$</p>
<p>Once we&rsquo;ve identified the optimal graph $\mathcal{G}$, the next step is to determine the most suitable embedding function, denoted as $\mathcal{f}_{\mathcal{G}}$. This function is responsible for mapping data points from a higher-dimensional space to a lower-dimensional one, akin to representing a 3D globe on a 2D map. The challenge lies in preserving the relationships between data points and the graph&rsquo;s structure during this transformation.</p>
<p>The second $\min$ guides us through the set $\mathcal{F}$ of potential embedding functions, helping us find the optimal $\mathcal{f}_{\mathcal{G}}$ that simplifies our data while maintaining its inherent structure and relationships. This balancing act is essential for effective machine learning.</p>
</li>
<li>
<p>The Third $\min$ finds the Optimal Embeddings $Z$:</p>
<p>$$
\min_{Z}
$$</p>
<p>With the graph $\mathcal{G}$ and the embedding function $\mathcal{f}_{\mathcal{G}}$ in place, the third $\min$ seeks to identify the optimal low-dimensional embeddings $Z$. These embeddings are representations of the data points in the lower-dimensional space that minimize the loss function defined by the summation in the equation.</p>
</li>
</ol>
<h3 id="simplified-loss-functions">(Simplified) Loss Functions<a hidden class="anchor" aria-hidden="true" href="#simplified-loss-functions">#</a></h3>
<p>Now, we get to the simplified loss function, or the part of the equation stated as:</p>
<p>$$
\sum_{(V_i, V_j) \in \mathcal{E}} b_{i,j}  || f_G (\mathbf{z}_i) - f_G (\mathbf{z}_j) ||^2
$$</p>
<p>The objective of the loss function is to measure the similarity or dissimilarity between two data points in the graph $\mathcal{G}$. As we traverse the graph&rsquo;s edges represented by $(V_i, V_j)$ in the set $\mathcal{E}$, the loss function computes the difference between the embeddings of these connected data points.</p>
<p><strong>Graph-Based Weights</strong>:</p>
<p>The loss function incorporates the notion of graph-based weights $b_{i,j}$. These weights allow us to assign different levels of importance to the edges connecting data points within the graph $\mathcal{G}$. By introducing these weights, we can emphasize or de-emphasize certain relationships, depending on their significance in the overall data representation.</p>
<figure style="text-align: center;" id="fig8">
    <img src="images/fig-08.png" width="70%" alt="Fig. 8" style="display: inline-block;">
    <figcaption align="center"><i>Fig. 8</i>. A brief overview of how edges and nodes interract</figcaption>
</figure>
<p>In the above (<a href="#fig8">Fig. 8</a>), we can define $\varepsilon_{i,j}$ as the edge connecting two nodes $V_i$ and $V_j$. The weight $b_{i,j}$ is a measure of the importance of this edge in the overall graph $\mathcal{G}$. The higher the weight, the more significant the edge, and vice versa. This concept is similar to the notion of <em>edge strength</em> in social networks, where the weight of an edge represents the strength of the relationship between any two individuals.</p>
<p><strong>Optimizing Low-Dimensional Embeddings</strong>:</p>
<p>The central aim of Reverse Graph Embeddings is to minimize the loss function. As the triple nested optimization progresses, the graph $\mathcal{G}$ and the embedding function $\mathcal{f}_{\mathcal{G}}$ are fine-tuned to achieve the optimal low-dimensional embeddings $Z$. These embeddings represent the data in a compact and meaningful manner in the lower-dimensional space.</p>
<p><strong>Preserving Graph Structure</strong>:</p>
<p>Through the process of minimizing the loss function, the algorithm ensures that the relationships between data points observed in the original graph $\mathcal{G}$ are maintained in the lower-dimensional space. This preservation of graph structure is essential for extracting meaningful insights and knowledge from the data.</p>
<h3 id="complete-loss-function-and-visualization">Complete Loss Function and Visualization<a hidden class="anchor" aria-hidden="true" href="#complete-loss-function-and-visualization">#</a></h3>
<p>This is a simplified version of the complete Reverse Graph Embedding, <em>only</em> considering graph structures in the latent (genotype) state, but not the observed phenotypes within the optimization parameters. To find a way to connect the high and low dimensions, RGE both ensures that</p>
<ol>
<li>The image under the function $\mathcal{f}_{\mathcal{G}}$ (points in the high-dimensional space as a <em>function</em> of the low-dimensional space) are close to one another as we described previously, multiplied by the weigbht of the associated edge $\mathcal{E}$, but save for the $\lambda$ hyperparameter which will be explained later on:</li>
</ol>
<p>$$
\frac{\lambda}{2} \sum_{(V_i, V_j) \in \mathcal{E}} b_{i,j}  || f_G (\mathbf{z}_i) - f_G (\mathbf{z}_j) ||^2
$$</p>
<ol start="2">
<li>An addended portion states that points which are neighbors on the low-dimensional principal graph are also &ldquo;neighbors&rdquo; in the input dimension, meaning for a given $\mathbf{z}_i$ (where $i$ is the index of the vector associated with a specific cell), the <em>estimated</em> phenotypes expressed must also be similar to the <em>real</em> phenotypes expressed by the cell in the high-dimension:</li>
</ol>
<p>$$
\sum_{i=1}^{N} || \mathbf{x}_i - f_G (\mathbf{z}_i) ||^2
$$</p>
<p>Resulting in the combined equation:</p>
<p>$$
\min_{\mathcal{G} \in G_b}{} \min_{f_{\mathcal{G}} \in \mathcal{F}}{} \min_{Z}{} \sum_{i=1}^{N} || \mathbf{x}_i - f_G (\mathbf{z}_i) ||^2
$$</p>
<p>$$
\text{+} \frac{\lambda}{2} \sum_{(V_i, V_j) \in \mathcal{E}} b_{i,j} || f_G (\mathbf{z}_i) - f_G (\mathbf{z}_j) ||^2
$$</p>
<p>Which when held together, represents the optimization constraints required to find and reduce a set of vectors associated with the high number of dimensions for each cell to an optimal number of lower dimensions for each cell. The below figure best summarizes the math behind what is taking place:</p>
<figure style="text-align: center;" id="fig9">
    <img src="images/fig-09.png" width="100%" alt="Fig. 9" style="display: inline-block;">
    <figcaption align="center"><i>Fig. 9</i>. Our completed, mathematically sound model of webs to waves</figcaption>
</figure>
<p>The image essentially illustrates that there is a function $\mathcal{f}_{\mathcal{G}}$ that maps lower-dimensional vectors in $\mathbb{R}^G$ (meaning they have $G$ number of dimensions) to a higher-dimensional space $\mathbb{R}^P$ (meaning they have $P$ number of expressed genes). The lower-dimensional graph, represented as a simpler web, is mapped to the higher-dimensional manifold (a smooth collection of points), depicted as a wave. This is a characteristic process in Monocle, a tool used for single-cell RNA-seq analysis.</p>
<p>And where does this lead in the end? In all honesty, the battle is only half over. The first half we just completed deals with creating the road that we will decide on how to drive down, finding the &ldquo;optimal&rdquo; (most likely) graph of latent variables that generates the phenotypes captured by whatever genes were sequenced. The next half finally buys a car and sets the direction of travel, lending to understanding a pseudo-temporal ordering of expression and how similarity in the lower-dimensional means for general similarity in time as well.</p>
<p>This will take us back, to revisit and understand further the analogy of the ball toss and how time can be used as a red-thread of fate to tie together the disparate destinies of each individual cell.</p>
<h2 id="the-conclusion">The Conclusion?<a hidden class="anchor" aria-hidden="true" href="#the-conclusion">#</a></h2>
<p>In this post, we&rsquo;ve journeyed through concepts surrounding latent models, gene expression, and the intricate systems that govern these phenomena. We&rsquo;ve used analogies that (I hope) would make even the most hardened scientist crack a smile, and visual aids that might make my high school art teacher finally be proud of me, all to help us understand these complex concepts.</p>
<p>We&rsquo;ve delved into the mathematical ocean of Reverse Graph Embeddings, a powerful tool for dimensionality reduction. We&rsquo;ve seen how these techniques can illuminate the relationship between genotypes and phenotypes, and how they can map high-dimensional data onto a lower-dimensional space while preserving the underlying structure. It&rsquo;s like fitting an elephant into a phone booth, but without violating any animal rights!</p>
<figure style="text-align: center;" id="elephant-phone-booth">
    <img src="images/elephant-phone-booth.jpg" width="70%" alt="Fig. 9" style="display: inline-block;">
    <figcaption align="center">You probably can't fit an elephant in a phone booth like this, so this elephant was probably dimensionally reduced!</figcaption>
</figure>
<p>We&rsquo;ve also explored how these techniques help us identify the optimal graph, the most suitable embedding function, and the best low-dimensional embeddings that represent the data. We&rsquo;ve looked at the role of graph-based weights in assigning different levels of importance to the edges connecting data points within the graph.</p>
<p>And let&rsquo;s not forget how the algorithm ensures that the relationships between data points observed in the original graph are maintained in the lower-dimensional space, like moving across the country but still finding the bandwith to keep in touch with your old friends.</p>
<figure style="text-align: center;" id="keeping-up">
    <img src="images/keeping-up.jpg" width="70%" alt="Fig. 9" style="display: inline-block;">
    <figcaption align="center">Keeping up with friends is important when you are somewhere different</figcaption>
</figure>
<p>The main paper on Monocle (<a href="https://cole-trapnell-lab.github.io/pdfs/papers/qiu-monocle2.pdf">Qiu et al.</a>), takes this concept even further. It explores how <em>trees</em> are employed to create a pseudo-temporal map of cells in various states of division, a journey which I will save for my next post. Yes, I&rsquo;m leaving you on a cliffhanger and sorry, not sorry.</p>
<p>Thank you for reading! And, please look forward to the next post that will set sail for high-dimensional oceans once more, when we cover the rest of graph learning for monocle. I promise it will be worth the wait!</p>
<figure style="text-align: center;" id="pirate-captain">
    <img src="images/pirate-captain.jpg" width="70%" alt="Fig. 9" style="display: inline-block;">
    <figcaption align="center">Who doesn't love the imagery of a cute pirate captain sailing the high seas?</figcaption>
</figure>
<h2 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h2>
<p>Cited as:</p>
<pre><code>Son, Sunny. (Jul 2023). &quot;Monocle - Reverse Graph Embeddings&quot;. Sunny's Notes. 
https://sunnyson.dev/notes/2023/07/monocle-reverse-graph-embeddings/.
</code></pre>
<p>Or, in BibTeX format:</p>
<pre tabindex="0"><code>@article{son-2023-monocle-pt1,
  title   = &quot;Monocle - Reverse Graph Embeddings&quot;,
  author  = &quot;Son, Sunny&quot;,
  journal = &quot;sunnyson.dev&quot;,
  year    = &quot;2023&quot;,
  month   = &quot;July&quot;,
  url     = &quot;https://sunnyson.dev/notes/2023/07/monocle-reverse-graph-embeddings/&quot;
}
</code></pre>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Trapnell, C. et al. <a href="https://www.nature.com/articles/nbt.2859">“The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells.&quot;</a> Nature Biotechnology 2014</p>
<p>[2] Qiu, Xiaojie. et al. <a href="https://www.nature.com/articles/nmeth.4402">“Reversed graph embedding resolves complex single-cell trajectories&quot;</a> Nature Methods 2017</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://sunnyson.dev/tags/monocle/">monocle</a></li>
      <li><a href="https://sunnyson.dev/tags/latent-model/">latent-model</a></li>
      <li><a href="https://sunnyson.dev/tags/dimensionality-reduction/">dimensionality-reduction</a></li>
      <li><a href="https://sunnyson.dev/tags/reverse-graph-embeddings/">reverse-graph-embeddings</a></li>
    </ul>
  </footer>
</article>
    </main>
    

<footer class="footer" style="padding-top: 18px; padding-bottom: 18px">
    
<div class="social-icons" style="padding-bottom: 0px;">
</div>

    <span>&copy; 2024 <a href="https://sunnyson.dev/">Sunny Son</a></span>

    
    <span>
        •
        <a href="/privacy">Privacy Policy</a>
    </span>
    
    <span>
        
        •
        Powered by
        
        <a href="https://gohugo.io/">Hugo</a>, 
        <a href="https://github.com/adityatelange/hugo-PaperMod/">PaperMod</a>, &
        <a href="https://en.wikipedia.org/wiki/Love">💓</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script>
    
    
    let b = document.querySelector("#menu-trigger")
    let m = document.querySelector(".menu")
    b.addEventListener("click", function () {
        m.classList.toggle("hidden")
    });
    
    document.body.addEventListener('click', function (event) {
        if (!b.contains(event.target)) {
            m.classList.add("hidden")
        }
    });
    
</script>


<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
    
</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>








</body>

</html>
